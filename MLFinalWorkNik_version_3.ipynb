{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "submit:\n",
    "1. Itamar Nakar\n",
    "itamar.nakar@gmail.com\n",
    "2. Nikita nedostoup\n",
    "336495593\n",
    "nikolos7771@gmail.com\n",
    "3. Lirom Mishan\n",
    "208199398\n",
    "lyrwmmsn@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "-gPx8a1Bam1v"
   },
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "NEGATIVE_SAMPLES = 9  # Negative samples per pair\n",
    "WINDOW_SIZE = 3  # Window size for context words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import data by using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "54TaLz_yH0p9",
    "outputId": "e0366849-4301-4356-b72a-d3878aa869cb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Truly is \"Jewel of the Upper Wets Side\"</td>\n",
       "      <td>Stayed in a king suite for 11 nights and yes i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My home away from home!</td>\n",
       "      <td>On every visit to NYC, the Hotel Beacon is the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great Stay</td>\n",
       "      <td>This is a great property in Midtown. We two di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Modern Convenience</td>\n",
       "      <td>The Andaz is a nice hotel in a central locatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Its the best of the Andaz Brand in the US....</td>\n",
       "      <td>I have stayed at each of the US Andaz properti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>One of NYC's Best Hotels, Hands Down</td>\n",
       "      <td>Excellent staff (they remembered our names fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Apartment living for less than most 5 stars</td>\n",
       "      <td>I stayed at the Setai for 3 nights last week, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lovely stay at The Chatwal</td>\n",
       "      <td>My husband and I stayed at The Chatwal for 9 n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Exclusive boutique hôtel</td>\n",
       "      <td>Wonderful boutique hotel located next to Times...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A Nice Stay for NYC!</td>\n",
       "      <td>This hotel is a nice stay for NYC because the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           title  \\\n",
       "0        Truly is \"Jewel of the Upper Wets Side\"   \n",
       "1                        My home away from home!   \n",
       "2                                     Great Stay   \n",
       "3                             Modern Convenience   \n",
       "4  Its the best of the Andaz Brand in the US....   \n",
       "5           One of NYC's Best Hotels, Hands Down   \n",
       "6    Apartment living for less than most 5 stars   \n",
       "7                     Lovely stay at The Chatwal   \n",
       "8                       Exclusive boutique hôtel   \n",
       "9                           A Nice Stay for NYC!   \n",
       "\n",
       "                                                text  \n",
       "0  Stayed in a king suite for 11 nights and yes i...  \n",
       "1  On every visit to NYC, the Hotel Beacon is the...  \n",
       "2  This is a great property in Midtown. We two di...  \n",
       "3  The Andaz is a nice hotel in a central locatio...  \n",
       "4  I have stayed at each of the US Andaz properti...  \n",
       "5  Excellent staff (they remembered our names fro...  \n",
       "6  I stayed at the Setai for 3 nights last week, ...  \n",
       "7  My husband and I stayed at The Chatwal for 9 n...  \n",
       "8  Wonderful boutique hotel located next to Times...  \n",
       "9  This hotel is a nice stay for NYC because the ...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "path_to_df = r\"C:\\Users\\avim0\\OneDrive - Bar-Ilan University - Students\\Notability\\שנה ג סמסטר ב\\למידת מכונה\\פרוייקט סופי\\review_230k_.parquet\"\n",
    "df = pd.read_parquet(path_to_df)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_qDcsLrYSfA"
   },
   "source": [
    "This part combine the title and the text together so the text will followed by the title in every row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "id": "NgyYTqFVIoYR",
    "outputId": "03cb3e93-e5b4-4a4a-8a3a-4bba8db20110"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avim0\\AppData\\Local\\Temp\\ipykernel_14096\\3624023226.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  processed_data['combined'] = processed_data['combined'].str.split('.')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Truly is \"Jewel of the Upper Wets Side\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stayed in a king suite for 11 nights and yes i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Our room was on the 20th floor overlooking Bro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Room was quite with no noise evident from the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was great to be able to open windows when w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The beds, including the fold out sofa bed, wer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wi-fi access worked like a dream with only one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The location close to the 72nd Street subway s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>It is fabulous to have the kitchen with cookin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This is the second time that members of the pa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            combined\n",
       "0            Truly is \"Jewel of the Upper Wets Side\"\n",
       "1  Stayed in a king suite for 11 nights and yes i...\n",
       "2  Our room was on the 20th floor overlooking Bro...\n",
       "3  Room was quite with no noise evident from the ...\n",
       "4  It was great to be able to open windows when w...\n",
       "5  The beds, including the fold out sofa bed, wer...\n",
       "6  Wi-fi access worked like a dream with only one...\n",
       "7  The location close to the 72nd Street subway s...\n",
       "8  It is fabulous to have the kitchen with cookin...\n",
       "9  This is the second time that members of the pa..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def create_dataset(data: pd.DataFrame):\n",
    "    \"\"\"create our dataset\"\"\"\n",
    "\n",
    "    # Combine 'title' and 'text' columns into a single 'combined' column\n",
    "    data['combined'] = data['title'] + '. ' + data['text']\n",
    "\n",
    "    processed_data = data[['combined']]\n",
    "\n",
    "    processed_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    processed_data['combined'] = processed_data['combined'].str.split('.')\n",
    "\n",
    "    # Use explode to create a new row for each sentence\n",
    "    processed_data = processed_data.explode('combined')\n",
    "\n",
    "    # Remove spaces from the sentences\n",
    "    processed_data['combined'] = processed_data['combined'].str.strip()\n",
    "\n",
    "    # Drop empty rows \n",
    "    processed_data = processed_data[processed_data['combined'] != '']\n",
    "    # Reset index\n",
    "    processed_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "dataset = create_dataset(df)\n",
    "dataset.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yllequ8dasy"
   },
   "source": [
    "### clean and standardize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0e3KGF7pv1_"
   },
   "source": [
    "Delete foreign language Words, and it's punctuation\n",
    "we ignored languages which are not english becuase we can't set rules for a lot of languages that have different rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 95
    },
    "id": "v9Ua3BBUnd0n",
    "outputId": "6d5843ce-7e6c-413c-ec82-83ff14e8f06a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\avim0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                     truly is jewel of the upper side\n",
      "1    stayed in a king suite for nights and yes it u...\n",
      "2    our room was on the floor broadway and the mad...\n",
      "3    room was quite with no noise evident from the ...\n",
      "4    it was great to be able to open when we fresh ...\n",
      "Name: combined, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "# Download the list of English words from NLTK\n",
    "nltk.download('words')\n",
    "english_vocab = set(words.words())\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Converts text to lowercase, removes punctuation (except $), and splits it into words.\n",
    "    \"\"\"\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s\\$]', '', text)  # Remove punctuation except $\n",
    "    return text.split()  # Split text into words\n",
    "\n",
    "def filter_foreign_words(dataset):\n",
    "    \"\"\"\n",
    "    Removes non-English words from the 'combined' column and stores the result in a new column 'english_only'.\n",
    "    \"\"\"\n",
    "    def filter_non_english_words(text):\n",
    "        words = clean_text(text)\n",
    "        # Keep only English words and the $ symbol\n",
    "        return ' '.join([word for word in words if word in english_vocab or word == '$'])\n",
    "\n",
    "    # Create a new column 'english_only' with only English words\n",
    "    dataset['combined'] = dataset['combined'].apply(filter_non_english_words)\n",
    "    return dataset\n",
    "\n",
    "# Apply the function to the dataset to remove foreign words\n",
    "dataset = filter_foreign_words(dataset)\n",
    "\n",
    "# Display the result with the new 'english_only' column\n",
    "print(dataset['combined'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJ6_zNA6qCyK"
   },
   "source": [
    "Replace rare words with the token '$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kQIdFz7_qBDt",
    "outputId": "c1dd63ec-0d3f-443f-e9da-163526905998"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from collections import Counter\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Converts text to lowercase, removes punctuation (except $), and splits it into words.\n",
    "    \"\"\"\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s\\$]', '', text)  # Remove punctuation except $\n",
    "    return text.split()  # Split text into words\n",
    "\n",
    "def replace_rare_words_with_dollar(dataset, min_frequency=3):\n",
    "    \"\"\"\n",
    "    Replaces rare English words in the 'combined' column with the $ symbol.\n",
    "    \"\"\"\n",
    "    # Collect all words from the 'combined' column\n",
    "    all_words = []\n",
    "    for text in dataset['combined']:\n",
    "        words = clean_text(text)\n",
    "        all_words.extend(words)\n",
    "\n",
    "    # Count word frequencies\n",
    "    word_freq = Counter(all_words)\n",
    "\n",
    "    # Replace rare words with the $ symbol\n",
    "    def replace_rare_words(text):\n",
    "        words = clean_text(text)\n",
    "        # Keep words with frequency >= min_frequency, otherwise replace with $\n",
    "        return ' '.join([word if word_freq[word] >= min_frequency or word == '$' else '$' for word in words])\n",
    "\n",
    "    # Create a new column 'replaced_rare' with rare words replaced by $\n",
    "    dataset['combined'] = dataset['combined'].apply(replace_rare_words)\n",
    "    return dataset\n",
    "\n",
    "min_frequency = 3  # Minimum frequency to keep a word\n",
    "\n",
    "# Apply the function to replace rare words with the $ symbol\n",
    "dataset = replace_rare_words_with_dollar(dataset, min_frequency=3)\n",
    "\n",
    "# Display the result with the 'replaced_rare' column\n",
    "print(dataset['combined'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replace symbols by the token '$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define a function to replace noisy words with '$'\n",
    "def replace_noisy_words_in_row(text):\n",
    "    words = text.split()  # Split the text into individual words\n",
    "    return ' '.join([word if re.match(r'^[a-zA-Z]+$', word) else '$' for word in words])\n",
    "\n",
    "# Apply the function to the 'combined' column in the dataset\n",
    "dataset['combined'] = dataset['combined'].apply(replace_noisy_words_in_row)\n",
    "\n",
    "# Display the first few rows\n",
    "print(dataset['combined'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDv7wfHQ5pn4"
   },
   "source": [
    "In this script we replace different types of stop words with the token '$'. We use nltk.courpus that collect many different stpowords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AxYiNKY17_ql",
    "outputId": "2871ccfd-ee1c-4eca-95e0-c72f18de209d"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # for extended WordNet data\n",
    "\n",
    "# Initialize stopwords set and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_and_normalize_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and normalizes the text by:\n",
    "    - Lowercasing\n",
    "    - Removing punctuation\n",
    "    - Removing numbers\n",
    "    - Replacing stopwords with '$'\n",
    "    - Applying lemmatization\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove stopwords and apply lemmatization\n",
    "    words = text.split()\n",
    "    lemmatized_words = [\n",
    "        lemmatizer.lemmatize(word) if word not in stop_words else '$'\n",
    "        for word in words\n",
    "    ]\n",
    "\n",
    "    return ' '.join(lemmatized_words)  # Join the lemmatized words back into a single string\n",
    "\n",
    "def prepare_and_normalize_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Applies the cleaning and normalization function to the entire dataset.\n",
    "    \"\"\"\n",
    "    # Apply the cleaning function to each sentence in the 'combined' column\n",
    "    dataset['combined'] = dataset['combined'].apply(lambda x: clean_and_normalize_text(x))\n",
    "    return dataset\n",
    "\n",
    "dataset = prepare_and_normalize_dataset(dataset)\n",
    "\n",
    "# Output the cleaned and normalized dataset\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-uMwgO7KdC6"
   },
   "source": [
    "Here we generate word pairs, because some of the the pairs are compound nouns. for example 'bus station'. It helps us reduce the size of the library and improve the running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vNLyB87MQ8K5",
    "outputId": "ed1bb3b3-2301-4c0e-d781-63d84d6dc13d"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def generate_word_pairs(text, window_size):\n",
    "    \"\"\"\n",
    "    Generate word pairs from the text within a given window size, skipping pairs containing '$'.\n",
    "    param text: Input text\n",
    "    param window_size: Number of words to consider around the target word\n",
    "    return: List of word pairs\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Split into words\n",
    "    words = text.split()\n",
    "\n",
    "    # List to store word pairs\n",
    "    word_pairs = []\n",
    "\n",
    "    # Iterate over each word and create pairs with it's neighbors\n",
    "    for idx, word in enumerate(words):\n",
    "        if '$' in word:\n",
    "            continue  # Skip if the target word contains '$'\n",
    "\n",
    "        start = max(0, idx - window_size)\n",
    "        end = min(len(words), idx + window_size + 1)\n",
    "\n",
    "        # Create pairs: current word + its neighbors, skip if neighbor contains '$'\n",
    "        for neighbor in words[start:idx] + words[idx+1:end]:\n",
    "            if '$' in neighbor:\n",
    "                continue  # Skip if the neighbor word contains '$'\n",
    "            word_pairs.append((word, neighbor))\n",
    "\n",
    "    return word_pairs\n",
    "\n",
    "def identify_phrases(word_pairs, threshold=25):\n",
    "    \"\"\"\n",
    "    Identify frequent word pairs (phrases) and return the modified text.\n",
    "\n",
    "    param word_pairs: List of word pairs from the text\n",
    "    param threshold: Frequency threshold to consider a word pair as a phrase\n",
    "    return: Dictionary of word pairs that should be treated as phrases\n",
    "    \"\"\"\n",
    "    # Count occurrences of word pairs\n",
    "    pair_count = defaultdict(int)\n",
    "    for pair in word_pairs:\n",
    "        pair_count[pair] += 1\n",
    "\n",
    "    # Select pairs that occur more than the threshold\n",
    "    phrases = {pair: '_'.join(pair) for pair, count in pair_count.items() if count >= threshold}\n",
    "\n",
    "    return phrases\n",
    "\n",
    "def replace_with_phrases(text, phrases):\n",
    "    \"\"\"\n",
    "    Replace frequent word pairs with phrases in the text.\n",
    "\n",
    "    param text: Input text\n",
    "    param phrases: Dictionary of word pairs and corresponding phrases\n",
    "    return: Modified text with phrases\n",
    "    \"\"\"\n",
    "    # Clean text (remove punctuation and lowercase, but keep $)\n",
    "    text = re.sub(r'[^\\w\\s\\$]', '', text.lower())\n",
    "\n",
    "    # Replace frequent word pairs with phrases\n",
    "    for (word1, word2), phrase in phrases.items():\n",
    "        text = text.replace(f\"{word1} {word2}\", phrase)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Batch processing with the updated way of skip '$'\n",
    "def process_dataset_in_batches(dataset, batch_size=1024, window_size=1, threshold=25):\n",
    "    total_rows = len(dataset)\n",
    "    processed_data = pd.DataFrame()\n",
    "\n",
    "    # Process dataset in batches\n",
    "    for batch_start in range(0, total_rows, batch_size):\n",
    "        batch = dataset.iloc[batch_start:batch_start + batch_size].copy()  # Get a batch of rows\n",
    "        all_word_pairs = []\n",
    "\n",
    "        # Generate word pairs for each text in the 'combined' column\n",
    "        for text in batch['combined']:\n",
    "            word_pairs = generate_word_pairs(text, window_size)\n",
    "            all_word_pairs.extend(word_pairs)\n",
    "\n",
    "        # Identify frequent word pairs and create phrases\n",
    "        phrases = identify_phrases(all_word_pairs, threshold)\n",
    "\n",
    "        # Replace frequent word pairs with phrases in each text\n",
    "        batch['combined'] = batch['combined'].apply(lambda x: replace_with_phrases(x, phrases))\n",
    "\n",
    "        # Append processed batch to final dataset\n",
    "        processed_data = pd.concat([processed_data, batch], ignore_index=True)\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "# print dataset after reduced library size\n",
    "dataset = process_dataset_in_batches(dataset, batch_size=1024, window_size=1, threshold=25)\n",
    "\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate words frequencies of apperance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l1ICy-n0gE0u",
    "outputId": "00f659e6-9adc-4511-ba35-6d0eaf29c7eb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculate_word_frequencies(dataset):\n",
    "    \"\"\"\n",
    "    Calculate the frequency of each word in the dataset.\n",
    "\n",
    "    param dataset: Input DataFrame containing a 'combined' column with text\n",
    "    return: Dictionary with word frequencies\n",
    "    \"\"\"\n",
    "    word_count = defaultdict(int)\n",
    "\n",
    "    # Iterate over each text in the dataset\n",
    "    for text in dataset['combined']:\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            word_count[word] += 1\n",
    "\n",
    "    return word_count\n",
    "\n",
    "def subsample_frequent_words(dataset, t):\n",
    "    \"\"\"\n",
    "    Apply subsampling to frequent words in the dataset, keeping words with '$'.\n",
    "\n",
    "    param dataset: Input DataFrame containing a 'combined' column with text\n",
    "    param t: Threshold for subsampling frequent words\n",
    "    return: Modified dataset with subsampled text\n",
    "    \"\"\"\n",
    "    # Calculate word frequencies across the dataset\n",
    "    word_frequencies = calculate_word_frequencies(dataset)\n",
    "\n",
    "    # Total number of words in the dataset\n",
    "    total_words = sum(word_frequencies.values())\n",
    "\n",
    "    # Calculate the probability of keeping each word\n",
    "    word_probabilities = {}\n",
    "    for word, count in word_frequencies.items():\n",
    "        frequency = count / total_words\n",
    "        word_probabilities[word] = 1 - np.sqrt(t / frequency) if frequency > t else 1.0\n",
    "\n",
    "    # Apply subsampling to the 'combined' column\n",
    "    def subsample_text(text):\n",
    "        words = text.split()\n",
    "        # Keep words with '$' and apply subsampling to others\n",
    "        sampled_words = [\n",
    "            word if ('$' in word or np.random.rand() < word_probabilities.get(word, 1.0))\n",
    "            else '$'\n",
    "            for word in words\n",
    "        ]\n",
    "        return ' '.join(sampled_words)\n",
    "\n",
    "    # Apply the subsampling to the entire dataset\n",
    "    dataset['combined'] = dataset['combined'].apply(subsample_text)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# the dataset with the text in the 'combined' column\n",
    "dataset = subsample_frequent_words(dataset, t=1e-2)\n",
    "\n",
    "# Output subsampling\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cleans all words that have only one letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to replace one-letter words (excluding 'a' and 'I') with '$'\n",
    "def replace_one_letter_words(text):\n",
    "    # Replace one-letter words with a dollar symbol using regex\n",
    "    replaced_text = re.sub(r'\\b(?![aI]\\b)[a-zA-Z]\\b', '$', text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    replaced_text = re.sub(r'\\s+', ' ', replaced_text).strip()\n",
    "    return replaced_text\n",
    "\n",
    "# Apply the function to the 'combined' column in the dataset\n",
    "dataset['combined'] = dataset['combined'].apply(replace_one_letter_words)\n",
    "\n",
    "# Display a sample of the cleaned data\n",
    "print(dataset['combined'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create A vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a vocabulary from the words in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(dataset):\n",
    "    \"\"\"\n",
    "    Creates a vocabulary from unique words in the 'combined_normalized' column of the dataset.\n",
    "    \"\"\"\n",
    "    unique_words = set()\n",
    "\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for text in dataset['combined']:\n",
    "        words = text.split()  # The text is already normalized, so just split into words\n",
    "        unique_words.update(words)\n",
    "\n",
    "    # Create a vocabulary dictionary that maps words to unique indexes\n",
    "    unique_words_dict = {word: idx for idx, word in enumerate(unique_words)}\n",
    "\n",
    "    return unique_words_dict\n",
    "\n",
    "# Create the vocabulary from the normalized text\n",
    "vocabulary = create_vocabulary(dataset)\n",
    "\n",
    "# Print the first few rows of the dataset and the vocabulary size\n",
    "print(dataset[['combined']].head())\n",
    "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoding the words in the dataset to vectors using the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4X8iaJzbgvfY",
    "outputId": "36c5a3c8-be4c-40dd-b58a-829e242769cf"
   },
   "outputs": [],
   "source": [
    "def encode_text(text, vocabulary):\n",
    "    \"\"\"\n",
    "    Encodes a text based on the provided vocabulary.\n",
    "    Any word not in the vocabulary will be skipped.\n",
    "    \"\"\"\n",
    "    words = text.lower().replace(\",\", \"\").replace(\"!\", \"\").split()\n",
    "    return [vocabulary.get(word,  ) for word in words if word in vocabulary]\n",
    "\n",
    "def encode_dataset(dataset, vocabulary):\n",
    "    \"\"\"\n",
    "    Encodes the entire dataset using the provided vocabulary.\n",
    "    Applies the encode_text function to each sentence in the 'combined' column.\n",
    "    \"\"\"\n",
    "    # Apply the encode_text function to all rows in the 'combined' column\n",
    "    dataset['encoded'] = dataset['combined'].apply(lambda x: encode_text(x, vocabulary))\n",
    "    return dataset\n",
    "\n",
    "# create the vocabulary\n",
    "vocabulary = create_vocabulary(dataset)\n",
    "\n",
    "# encode the entire dataset\n",
    "encoded_dataset = encode_dataset(dataset , vocabulary)\n",
    "\n",
    "# Print the first few rows\n",
    "print(encoded_dataset[['combined', 'encoded']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAd3nhH8skDB"
   },
   "source": [
    "cleaning empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FrDp0UwdXR4e",
    "outputId": "876d2f03-f317-4a39-ad2c-619c9a91c78f"
   },
   "outputs": [],
   "source": [
    "# the index of the $ symbol is known\n",
    "encoded_dollar_sign = vocabulary.get('$')\n",
    "\n",
    "# Find rows with empty or invalid values in the 'encoded' column\n",
    "empty_or_invalid_rows_encoded = encoded_dataset[\n",
    "    encoded_dataset['encoded'].isna() |  # Empty values\n",
    "    (encoded_dataset['encoded'].apply(len) == 0) |  # Empty encoded lists\n",
    "    (encoded_dataset['encoded'].apply(lambda x: len(x) == 1 and x[0] == encoded_dollar_sign))  # Only $ symbol\n",
    "]\n",
    "\n",
    "# Print the number of rows with empty or invalid encoded values\n",
    "print(f\"Number of rows with empty or invalid encoded values: {len(empty_or_invalid_rows_encoded)}\")\n",
    "\n",
    "# Show the first few rows with empty or invalid encoded values\n",
    "print(empty_or_invalid_rows_encoded.head())\n",
    "\n",
    "# Remove rows with empty or invalid encoded values\n",
    "encoded_dataset = encoded_dataset.drop(empty_or_invalid_rows_encoded.index).reset_index(drop=True)\n",
    "\n",
    "# Print the updated dataset size after cleaning\n",
    "print(f\"Dataset size after cleaning: {len(encoded_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Positive and Negative Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WN8IQznAC7MP",
    "outputId": "edf84677-ac45-4661-f84e-bd580f91d277"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "def generate_training_data_with_batching(encoded_dataset, number_of_neg_samples: int, batch_size: int, window_size: int):\n",
    "    \"\"\"\n",
    "    Generate positive and negative pairs using batching and optimized negative sampling.\n",
    "    \"\"\"\n",
    "    word_freq = defaultdict(int)\n",
    "\n",
    "    # Count the frequency of words, ignoring the `$` symbol\n",
    "    for sentence in encoded_dataset:\n",
    "        for word in sentence:\n",
    "            if word != encoded_dollar_sign:  # Ignore $ when counting frequencies\n",
    "                word_freq[word] += 1\n",
    "\n",
    "    total_words = sum(word_freq.values())\n",
    "    word_prob = np.array([(freq / total_words) ** 0.75 for freq in word_freq.values()])\n",
    "    words = np.array(list(word_freq.keys()))\n",
    "    word_prob /= word_prob.sum()\n",
    "\n",
    "    all_targets, all_contexts, all_labels = [], [], []\n",
    "\n",
    "    for batch_start in range(0, len(encoded_dataset), batch_size):\n",
    "        batch_sentences = encoded_dataset[batch_start:batch_start + batch_size]\n",
    "\n",
    "        batch_targets, batch_contexts = [], []\n",
    "\n",
    "        for sentence in batch_sentences:\n",
    "            sentence_length = len(sentence)\n",
    "\n",
    "            for i, target_word in enumerate(sentence):\n",
    "                if target_word == encoded_dollar_sign:\n",
    "                    continue  # Skip $ as the target word\n",
    "\n",
    "                # Form the full window (including $)\n",
    "                start = max(i - window_size, 0)\n",
    "                end = min(i + window_size + 1, sentence_length)\n",
    "\n",
    "                # The context includes all words except the current target word\n",
    "                context = list(sentence[start:i]) + list(sentence[i + 1:end])\n",
    "\n",
    "                # Remove the $ symbol from the context\n",
    "                context = [w for w in context if w != encoded_dollar_sign]\n",
    "\n",
    "                # organize the context to the correct size if it get out of the limits of the window size after removing $\n",
    "                context = context[:window_size]  # Keep only the nearest words\n",
    "\n",
    "                # Add target-context pairs\n",
    "                batch_targets.extend([target_word] * len(context))\n",
    "                batch_contexts.extend(context)\n",
    "\n",
    "        batch_targets = np.array(batch_targets)\n",
    "        batch_contexts = np.array(batch_contexts)\n",
    "\n",
    "        # Generate negative samples\n",
    "        num_positive = len(batch_targets)\n",
    "        negative_samples = np.random.choice(words, size=num_positive * number_of_neg_samples, p=word_prob)\n",
    "        negative_samples = negative_samples.reshape(num_positive, number_of_neg_samples)\n",
    "\n",
    "        # Create negative pairs\n",
    "        target_neg = np.repeat(batch_targets, number_of_neg_samples)\n",
    "        negative_words = negative_samples.flatten()\n",
    "\n",
    "        pairs_positive = np.stack([batch_targets, batch_contexts], axis=1)\n",
    "        pairs_negative = np.stack([target_neg, negative_words], axis=1)\n",
    "\n",
    "        pairs = np.concatenate([pairs_positive, pairs_negative], axis=0)\n",
    "        labels = np.concatenate([np.ones(len(pairs_positive), dtype=np.int32),\n",
    "                                 np.zeros(len(pairs_negative), dtype=np.int32)])\n",
    "\n",
    "        # Add results to global lists\n",
    "        all_targets.append(pairs[:, 0])\n",
    "        all_contexts.append(pairs[:, 1])\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    # Convert global lists to arrays\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    all_contexts = np.concatenate(all_contexts)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    return all_targets, all_contexts, all_labels\n",
    "\n",
    "# using the function\n",
    "number_of_neg_samples = 4\n",
    "window_size = 2\n",
    "batch_size = 16382\n",
    "\n",
    "# `encoded_dataset` \n",
    "targets, contexts, labels = generate_training_data_with_batching(\n",
    "    encoded_dataset['encoded'], number_of_neg_samples, batch_size, window_size\n",
    ")\n",
    "\n",
    "print(f\"Total pairs generated: {len(targets)}\")\n",
    "print(f\"Example target-context pairs and labels: {targets[:5]}, {contexts[:5]}, {labels[:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. we build here the skip gram model\n",
    "2. we define the input layers as targets and context words.\n",
    "3. convert input words to vectors.\n",
    "4. by dot product we calculate the relation between the vectors. \n",
    "5. In order to compile the model we use binary cross entropy and adam optimizer\n",
    "\n",
    "In the output:\n",
    "1. target_input an context_input are the input layers that represent the target word and the the context word.\n",
    "2. in the embedding we give every word a one hot vector.\n",
    "3. dot is the scalar multiply of vectors target_input and context_input\n",
    "4. dropout is cut off some of the neurons for regularization.\n",
    "5. dense is the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a model with Dropout and L2 regularization\n",
    "def create_skipgram_model_with_regularization(vocabulary_size, embedding_dim, dropout_rate, l2_lambda):\n",
    "    \"\"\"\n",
    "    Creates a Skip-Gram model with Dropout and L2 regularization.\n",
    "    \"\"\"\n",
    "    target_input = tf.keras.Input(shape=(1,), name='target_input')\n",
    "    context_input = tf.keras.Input(shape=(1,), name='context_input')\n",
    "\n",
    "    embedding = tf.keras.layers.Embedding(\n",
    "        input_dim=vocabulary_size,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=1,\n",
    "        name='embedding'\n",
    "    )\n",
    "\n",
    "    target_embedding = embedding(target_input)\n",
    "    context_embedding = embedding(context_input)\n",
    "\n",
    "    # Compute the dot product between the two embeddings\n",
    "    dot_product = tf.keras.layers.Dot(axes=-1)([target_embedding, context_embedding])\n",
    "\n",
    "    # Apply Dropout for regularization\n",
    "    dropout_output = tf.keras.layers.Dropout(dropout_rate)(dot_product) # Apply dropout to the dot product\n",
    "\n",
    "\n",
    "    # Output layer with sigmoid activation and regulizacion\n",
    "    output = tf.keras.layers.Dense(\n",
    "    1, activation='sigmoid',\n",
    "    kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)\n",
    "    )(dropout_output) # Pass the output of dropout to Dense layer\n",
    "\n",
    "\n",
    "    model = tf.keras.Model(inputs=[target_input, context_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define model parameters\n",
    "VOCABULARY_SIZE = len(vocabulary)\n",
    "EMBEDDING_DIM = 220\n",
    "\n",
    "# Create the model\n",
    "skipgram_model = create_skipgram_model_with_regularization(\n",
    "    vocabulary_size=VOCABULARY_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    dropout_rate=0.2,\n",
    "    l2_lambda=0.01\n",
    ")\n",
    "\n",
    "# Save the model for future use\n",
    "skipgram_model.save(\"skipgram_model_initial.h5\")\n",
    "print(\"Model created and saved.\")\n",
    "skipgram_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "train the model by use skip gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWohe4N94V4z"
   },
   "source": [
    "Here we are training and testing the model. we add early_stoping for stop the runing when the model is not improving anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the model\n",
    "skipgram_model = tf.keras.models.load_model(\"skipgram_model_initial.h5\", compile=False)\n",
    "optimizer = Adam(learning_rate=0.01) # decrethe lerning rate for avoinding to strong changing in weigths\n",
    "# Recompile the model to avoid issues with the optimizer\n",
    "skipgram_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    "  )\n",
    "\n",
    "# Ensure that the data has the correct shape\n",
    "targets = np.reshape(targets, (-1, 1))\n",
    "contexts = np.reshape(contexts, (-1, 1))\n",
    "labels = np.reshape(labels, (-1, 1))\n",
    "\n",
    "# Check the shape of the data\n",
    "print(f\"Targets shape: {targets.shape}\")\n",
    "print(f\"Contexts shape: {contexts.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "\n",
    "# Set up early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "   monitor='val_loss',\n",
    "   patience=5,\n",
    "   restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = skipgram_model.fit(\n",
    "   [targets, contexts],\n",
    "   labels,\n",
    "   epochs=17,\n",
    "   batch_size=32768,\n",
    "   validation_split=0.2,\n",
    "\n",
    "   callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "skipgram_model.save(\"word2vec_model_with_dropout_trained.h5\")\n",
    "print(\"Training completed and model saved.\")\n",
    "skipgram_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ri_vkhRHvgPx"
   },
   "source": [
    "__________________________NEW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the embeddings to external payrrow map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FR1meZTWl-i0",
    "outputId": "0d5f8769-6d30-44eb-fa26-87e6b81522f1"
   },
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "\n",
    "def save_embeddings(model, vocabulary: list, path: str):\n",
    "    \"\"\"\n",
    "    Save the embeddings to a Parquet file using pyarrow.\n",
    "\n",
    "    param model: The trained TensorFlow model\n",
    "    param vocabulary: List of tokens (words)\n",
    "    param path: Path to save the embeddings file (without extension)\n",
    "\n",
    "    The function will extract the embedding weights and save them in a PyArrow Table format:\n",
    "    <word> <embedding_vector>\n",
    "    \"\"\"\n",
    "    # Get the embedding layer weights\n",
    "    embedding_layer = model.get_layer('embedding')\n",
    "    embeddings = embedding_layer.get_weights()[0]  # The weights are in the first index of get_weights()\n",
    "\n",
    "    # Ensure the embeddings and vocabulary size match\n",
    "    if len(vocabulary) != embeddings.shape[0]:\n",
    "        print(len(vocabulary))\n",
    "        print((embeddings.shape[0]))\n",
    "        raise ValueError(\"Vocabulary size and embeddings size do not match!\")\n",
    "        \n",
    "    # Convert the embeddings to a list of lists (for each word)\n",
    "    embedding_list = embeddings.tolist()\n",
    "\n",
    "    # Create a PyArrow table with two columns: \"word\" and \"embedding\"\n",
    "    table = pa.Table.from_pydict({\n",
    "        \"word\": vocabulary,\n",
    "        \"embedding\": embedding_list\n",
    "    })\n",
    "\n",
    "    # Save the table as a Parquet file\n",
    "    pq.write_table(table, f\"{path}.parquet\")\n",
    "\n",
    "    print(f\"Embeddings saved to {path}.parquet\")\n",
    "\n",
    "save_embeddings(skipgram_model, vocabulary, f\"word2vec_embedding_w{WINDOW_SIZE}_ns{NEGATIVE_SAMPLES}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuvhL-1_aB27"
   },
   "source": [
    "### Find Most similar\r\n",
    "Note that this must be efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "return list of the k most similar vectors to a word with regards to cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XUTjqjqzuITm",
    "outputId": "e6d34247-7bc9-4f48-c835-913365c73f1e"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple \n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two vectors.\n",
    "\n",
    "    param: vec1: First vector (numpy array)\n",
    "    param: vec2: Second vector (numpy array)\n",
    "    return: Cosine similarity as a float\n",
    "    \"\"\"\n",
    "    # Compute the dot product of the two vectors\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "\n",
    "    # Compute the L2 norms (magnitudes) of the vectors\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "\n",
    "    # Compute the cosine similarity\n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        # If either vector has zero magnitude, cosine similarity is undefined\n",
    "        return 0.0\n",
    "\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "def find_most_similar(model, word: str, vocabulary: Dict[str, int], k: int = 10) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Find most similar tokens to the input word.\n",
    "\n",
    "    param: model: the trained Word2Vec model\n",
    "    param: word: the word to find the most similar words to\n",
    "    param: vocabulary: dictionary mapping words to indexes\n",
    "    param: k: number of most similar words to return\n",
    "    return: List of tuples containing similar words and their cosine similarity scores\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the embeddings from the model\n",
    "    embedding_layer = model.get_layer('embedding') \n",
    "    embeddings = embedding_layer.get_weights()[0]  # (vocab_size, embedding_dim)\n",
    "\n",
    "    # Get the index of the input word in the vocabulary\n",
    "    if word not in vocabulary:\n",
    "        raise ValueError(f\"Word '{word}' not found in the vocabulary\")\n",
    "\n",
    "    word_idx = vocabulary[word]  # Get the index of the input word\n",
    "\n",
    "    # Get the embedding of the input word\n",
    "    word_embedding = embeddings[word_idx]  # Embedding of the input word\n",
    "\n",
    "    # Compute cosine similarity between the input word and all other words\n",
    "    similarities = []\n",
    "\n",
    "    for other_word, other_idx in vocabulary.items():\n",
    "        if other_word == word:\n",
    "            continue  # Skip the input word itself\n",
    "        other_embedding = embeddings[other_idx]\n",
    "        similarity = cosine_similarity(word_embedding, other_embedding)\n",
    "        similarities.append((other_word, similarity))\n",
    "\n",
    "    # Sort the words by similarity in descending order\n",
    "    most_similar = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return the top k most similar words\n",
    "    return most_similar[:k]\n",
    "putinig_word = 'hotel'\n",
    "print(f\"My word is: {putinig_word} \")\n",
    "print(find_most_similar(skipgram_model, putinig_word, vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TS8LsofmaHMZ"
   },
   "source": [
    "### Dimentionality Reduction\r\n",
    "visualize some clusters (pick some subset of words to show the labels for)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will decrease the number of dimensions of the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = skipgram_model.get_layer('embedding')\n",
    "X = embeddings = embedding_layer.get_weights()[0]\n",
    "words = vocabulary\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "n_components = 2\n",
    "pca = PCA(n_components=n_components)\n",
    "X_reduced = pca.fit_transform(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize a defaultdict to count word frequencies\n",
    "word_count = defaultdict(int)\n",
    "\n",
    "# Iterate over each row in the dataset\n",
    "for text in encoded_dataset['combined']:\n",
    "    # Convert text to lowercase and remove punctuation\n",
    "    words_in_text = text.lower().replace(\",\", \"\").replace(\"!\", \"\").split()\n",
    "\n",
    "    # Count occurrences of words that are in the English vocabulary\n",
    "    for word in words_in_text:\n",
    "        if word in english_vocab:  # Check if the word is an English word\n",
    "            word_count[word] += 1\n",
    "\n",
    "# Convert the dictionary to a list of tuples (word, count), and sort by count\n",
    "word_count_list = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Select the top n most frequent words\n",
    "n = 40  \n",
    "top_n_words = word_count_list[:n]\n",
    "\n",
    "print(top_n_words)\n",
    "# Assume X_reduced is the (n_words, 2) array after PCA reduction to 2D\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Extract word indexes from vocabulary\n",
    "words = [word for word, count in top_n_words]\n",
    "indices = [vocabulary[word] for word in words]  # Get the indexes of top_n_words from the vocabulary\n",
    "\n",
    "plt.scatter(X_reduced[indices, 0], X_reduced[indices, 1], s=10, cmap='viridis')\n",
    "\n",
    "# Annotate the points with words from top_n_words\n",
    "for word, idx in zip(words, indices):\n",
    "    plt.annotate(word, (X_reduced[idx, 0], X_reduced[idx, 1]))\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('2D PCA Word Embedding Visualization for Top n Words')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "\n",
    "# Save and show the plot\n",
    "plt.savefig('word_embeddings_pca.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the clusters in the dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heare we chek varins, for chosing the best number of components\n",
    "Heare we use elbow metod for disede with number of clusters we will chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Extract embeddings from the trained model and standardize them\n",
    "embedding_layer = skipgram_model.get_layer('embedding')\n",
    "X = embeddings = embedding_layer.get_weights()[0]  # Extract embeddings\n",
    "words = vocabulary  # Dictionary with word indexes\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Dimensionality reduction using PCA\n",
    "n_components = 200  # Number of components to retain 90-95% of the variance\n",
    "pca = PCA(n_components=n_components)\n",
    "X_reduced = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Check explained variance\n",
    "explained_variance = pca.explained_variance_ratio_.sum()\n",
    "print(f'Explained variance with {n_components} components: {explained_variance:.2f}')\n",
    "\n",
    "# Apply KMeans for clustering\n",
    "n_clusters = 2  # Number of clusters \n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X_reduced)\n",
    "\n",
    "# Evaluate clustering quality using metrics\n",
    "sil_score = silhouette_score(X_reduced, cluster_labels)\n",
    "ch_score = calinski_harabasz_score(X_reduced, cluster_labels)\n",
    "db_score = davies_bouldin_score(X_reduced, cluster_labels)\n",
    "\n",
    "print(f'Silhouette Score: {sil_score}')\n",
    "print(f'Calinski-Harabasz Index: {ch_score}')\n",
    "print(f'Davies-Bouldin Index: {db_score}')\n",
    "\n",
    "# Visualize clusters in 2D\n",
    "plt.figure(figsize=(10, 10))\n",
    "scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=cluster_labels, cmap='viridis', s=20)\n",
    "plt.colorbar(scatter, label='Cluster Labels')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('Cluster Visualization')\n",
    "plt.grid(True)\n",
    "\n",
    "# Annotate some words on the plot\n",
    "words_to_annotate = ['hotel', 'property', 'good', 'room']  # Words for annotation\n",
    "for word in words_to_annotate:\n",
    "    if word in vocabulary:  # Check if the word is in the vocabulary\n",
    "        idx = vocabulary[word]\n",
    "        plt.annotate(word, (X_reduced[idx, 0], X_reduced[idx, 1]), fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Elbow method to determine the optimal number of clusters\n",
    "distortions = []\n",
    "for k in range(2, 10):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(X_reduced)\n",
    "    distortions.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(2, 10), distortions, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
